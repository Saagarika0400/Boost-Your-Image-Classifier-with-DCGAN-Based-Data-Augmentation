{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import scipy.io\n",
        "import shutil\n",
        "\n",
        "# URLs\n",
        "IMG_URL   = 'http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz'\n",
        "LBL_URL   = 'http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat'\n",
        "\n",
        "# Paths\n",
        "RAW_DIR   = './data/flowers_raw'\n",
        "TGZ_PATH  = os.path.join(RAW_DIR, '102flowers.tgz')\n",
        "MAT_PATH  = os.path.join(RAW_DIR, 'imagelabels.mat')\n",
        "EXTRACTED = os.path.join(RAW_DIR, 'jpg')\n",
        "DST_DIR   = './data/flowers'\n",
        "\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "os.makedirs(DST_DIR, exist_ok=True)\n",
        "\n",
        "# Download\n",
        "urllib.request.urlretrieve(IMG_URL, TGZ_PATH)\n",
        "urllib.request.urlretrieve(LBL_URL, MAT_PATH)\n",
        "\n",
        "# Extract images\n",
        "with tarfile.open(TGZ_PATH, 'r:gz') as tar:\n",
        "    tar.extractall(path=RAW_DIR)\n",
        "\n",
        "# Load labels\n",
        "mat    = scipy.io.loadmat(MAT_PATH)\n",
        "labels = mat['labels'][0]   # array of length 8189, values 1–102\n",
        "\n",
        "# Organize into class subfolders\n",
        "for idx, cls in enumerate(labels, start=1):\n",
        "    cls_folder = os.path.join(DST_DIR, f\"{cls:03d}\")\n",
        "    os.makedirs(cls_folder, exist_ok=True)\n",
        "    src_img = os.path.join(EXTRACTED, f\"image_{idx:05d}.jpg\")\n",
        "    dst_img = os.path.join(cls_folder, f\"image_{idx:05d}.jpg\")\n",
        "    shutil.copy(src_img, dst_img)\n",
        "\n",
        "print(\"Data prep complete. Structured data in:\", DST_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnKPa1UVxldV",
        "outputId": "d7ef0347-e9ee-4ef4-f661-e5755836efcd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data prep complete. Structured data in: ./data/flowers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ——————————————— Config ———————————————\n",
        "data_dir    = './data/flowers'    # output of step 2\n",
        "out_dir     = './outputs'\n",
        "image_size  = 64\n",
        "batch_size  = 64\n",
        "z_dim       = 100\n",
        "ngf, ndf    = 64, 64\n",
        "num_epochs  = 50\n",
        "lr          = 2e-4\n",
        "beta1       = 0.5\n",
        "device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# ——————————————— DataLoader ———————————————\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
        "loader  = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnUYlNncxTeb",
        "outputId": "f2a87a5d-bb67-44f0-eafe-13e47fcd1a9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# —————————————— Weight init ——————————————\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if 'Conv' in classname:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif 'BatchNorm' in classname:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "metadata": {
        "id": "yiiYkHZkxbBX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ————————————— Generator —————————————\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, ngf*8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf*8), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf*4), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf*2), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf*2, ngf,   4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),   nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf, 3,       4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.main(x)"
      ],
      "metadata": {
        "id": "EtE68rLix7Xc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ————————————— Discriminator —————————————\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf*2), nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf*4), nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf*4, ndf*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf*8), nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf*8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.main(x).view(-1)"
      ],
      "metadata": {
        "id": "GrNo9jj4x_wt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ————————— Instantiate —————————\n",
        "netG = Generator().to(device); netG.apply(weights_init)\n",
        "netD = Discriminator().to(device); netD.apply(weights_init)\n",
        "criterion   = nn.BCELoss()\n",
        "optD        = optim.Adam(netD.parameters(), lr=lr, betas=(beta1,0.999))\n",
        "optG        = optim.Adam(netG.parameters(), lr=lr, betas=(beta1,0.999))\n",
        "fixed_noise = torch.randn(64, z_dim, 1, 1, device=device)"
      ],
      "metadata": {
        "id": "mTY-mCmfyC8s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ——————————— Training Loop ———————————\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    for i, (real, _) in enumerate(loader, 1):\n",
        "        b_size = real.size(0)\n",
        "        real, labels_real = real.to(device), torch.ones(b_size, device=device)\n",
        "        labels_fake       = torch.zeros(b_size, device=device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        netD.zero_grad()\n",
        "        out_real = netD(real)\n",
        "        lossD_real = criterion(out_real, labels_real)\n",
        "        noise     = torch.randn(b_size, z_dim, 1, 1, device=device)\n",
        "        fake_imgs = netG(noise)\n",
        "        out_fake  = netD(fake_imgs.detach())\n",
        "        lossD_fake= criterion(out_fake, labels_fake)\n",
        "        (lossD_real + lossD_fake).backward()\n",
        "        optD.step()\n",
        "\n",
        "        # Train Generator\n",
        "        netG.zero_grad()\n",
        "        out2 = netD(fake_imgs)\n",
        "        lossG= criterion(out2, labels_real)\n",
        "        lossG.backward()\n",
        "        optG.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"[{epoch}/{num_epochs}][{i}/{len(loader)}] \"\n",
        "                  f\"Loss_D: {(lossD_real+lossD_fake).item():.4f}  Loss_G: {lossG.item():.4f}\")\n",
        "\n",
        "    # save sample grid\n",
        "    with torch.no_grad():\n",
        "        grid = utils.make_grid(netG(fixed_noise).cpu(), padding=2, normalize=True)\n",
        "        utils.save_image(grid, f\"{out_dir}/epoch_{epoch:03d}.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnH1beSayGLv",
        "outputId": "46f8dd3c-b671-4f53-fd99-293d5ccd8bf9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/50][100/128] Loss_D: 0.3584  Loss_G: 6.9421\n",
            "[2/50][100/128] Loss_D: 0.2060  Loss_G: 4.0963\n",
            "[3/50][100/128] Loss_D: 0.3166  Loss_G: 6.1467\n",
            "[4/50][100/128] Loss_D: 0.3276  Loss_G: 5.2761\n",
            "[5/50][100/128] Loss_D: 0.8813  Loss_G: 5.8841\n",
            "[6/50][100/128] Loss_D: 0.4318  Loss_G: 5.6299\n",
            "[7/50][100/128] Loss_D: 1.0002  Loss_G: 1.8013\n",
            "[8/50][100/128] Loss_D: 0.9953  Loss_G: 4.7000\n",
            "[9/50][100/128] Loss_D: 0.6727  Loss_G: 3.5765\n",
            "[10/50][100/128] Loss_D: 0.7395  Loss_G: 2.2092\n",
            "[11/50][100/128] Loss_D: 0.5242  Loss_G: 3.6913\n",
            "[12/50][100/128] Loss_D: 0.4808  Loss_G: 3.2172\n",
            "[13/50][100/128] Loss_D: 1.8008  Loss_G: 1.4882\n",
            "[14/50][100/128] Loss_D: 0.7996  Loss_G: 1.6790\n",
            "[15/50][100/128] Loss_D: 0.8850  Loss_G: 1.5200\n",
            "[16/50][100/128] Loss_D: 1.5278  Loss_G: 4.0439\n",
            "[17/50][100/128] Loss_D: 0.8775  Loss_G: 2.5257\n",
            "[18/50][100/128] Loss_D: 0.9896  Loss_G: 4.8267\n",
            "[19/50][100/128] Loss_D: 1.0011  Loss_G: 1.4251\n",
            "[20/50][100/128] Loss_D: 2.1695  Loss_G: 1.6660\n",
            "[21/50][100/128] Loss_D: 0.6394  Loss_G: 3.7231\n",
            "[22/50][100/128] Loss_D: 0.5555  Loss_G: 3.2104\n",
            "[23/50][100/128] Loss_D: 0.4670  Loss_G: 2.9355\n",
            "[24/50][100/128] Loss_D: 0.4709  Loss_G: 4.2991\n",
            "[25/50][100/128] Loss_D: 0.4930  Loss_G: 2.6855\n",
            "[26/50][100/128] Loss_D: 0.3915  Loss_G: 2.6899\n",
            "[27/50][100/128] Loss_D: 0.8047  Loss_G: 1.9356\n",
            "[28/50][100/128] Loss_D: 0.6121  Loss_G: 1.7525\n",
            "[29/50][100/128] Loss_D: 0.5362  Loss_G: 2.6608\n",
            "[30/50][100/128] Loss_D: 0.6985  Loss_G: 5.1780\n",
            "[31/50][100/128] Loss_D: 0.4030  Loss_G: 3.1667\n",
            "[32/50][100/128] Loss_D: 0.4948  Loss_G: 2.3977\n",
            "[33/50][100/128] Loss_D: 0.5262  Loss_G: 2.0817\n",
            "[34/50][100/128] Loss_D: 0.9302  Loss_G: 1.6815\n",
            "[35/50][100/128] Loss_D: 0.5938  Loss_G: 2.2952\n",
            "[36/50][100/128] Loss_D: 0.3997  Loss_G: 3.6329\n",
            "[37/50][100/128] Loss_D: 0.7740  Loss_G: 3.4845\n",
            "[38/50][100/128] Loss_D: 0.5320  Loss_G: 1.7368\n",
            "[39/50][100/128] Loss_D: 0.6309  Loss_G: 2.7564\n",
            "[40/50][100/128] Loss_D: 0.5366  Loss_G: 3.2996\n",
            "[41/50][100/128] Loss_D: 0.6345  Loss_G: 6.2987\n",
            "[42/50][100/128] Loss_D: 0.7152  Loss_G: 1.2578\n",
            "[43/50][100/128] Loss_D: 0.4741  Loss_G: 3.1566\n",
            "[44/50][100/128] Loss_D: 0.6669  Loss_G: 5.2144\n",
            "[45/50][100/128] Loss_D: 0.6411  Loss_G: 1.5848\n",
            "[46/50][100/128] Loss_D: 0.8146  Loss_G: 1.6701\n",
            "[47/50][100/128] Loss_D: 0.7623  Loss_G: 3.4168\n",
            "[48/50][100/128] Loss_D: 0.3558  Loss_G: 3.7513\n",
            "[49/50][100/128] Loss_D: 0.5844  Loss_G: 1.8973\n",
            "[50/50][100/128] Loss_D: 0.3587  Loss_G: 3.7028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ————— Generate new images for augmentation —————\n",
        "netG.eval()\n",
        "n_new = 500\n",
        "z     = torch.randn(n_new, z_dim, 1, 1, device=device)\n",
        "with torch.no_grad():\n",
        "    gen = netG(z).cpu()\n",
        "\n",
        "aug_dir = os.path.join(out_dir, 'augmented')\n",
        "# Create a subdirectory for the augmented images (a single class folder)\n",
        "aug_class_dir = os.path.join(aug_dir, '000') # Using '000' as a placeholder class\n",
        "os.makedirs(aug_class_dir, exist_ok=True)\n",
        "\n",
        "for idx, img in enumerate(gen, 1):\n",
        "    # Save the image inside the class subdirectory\n",
        "    utils.save_image(img, f\"{aug_class_dir}/aug_{idx:04d}.png\", normalize=True)\n",
        "\n",
        "print(\"Done! Augmented images in:\", aug_class_dir) # Updated print statement"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d8P8w7R8qee",
        "outputId": "c4da8487-d141-4bb8-8549-ee39e0758062"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Augmented images in: ./outputs/augmented/000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Path to the folder you just filled with images\n",
        "aug_dir = os.path.join(out_dir, 'augmented')\n",
        "\n",
        "# Path (without extension) for the zip archive\n",
        "zip_base = os.path.join(out_dir, 'augmented_images')\n",
        "\n",
        "# This will create 'augmented_images.zip' in your out_dir\n",
        "shutil.make_archive(zip_base, 'zip', root_dir=aug_dir)\n",
        "\n",
        "print(f\"Augmented images zipped at {zip_base}.zip\")"
      ],
      "metadata": {
        "id": "r53UnzrGyM7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b757079-c50d-49ce-938b-ff0e5b179626"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented images zipped at ./outputs/augmented_images.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ez09Eq5p-Aft"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}